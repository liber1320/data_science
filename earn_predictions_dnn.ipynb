{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earning predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the project was to predict if a person would earn more than 50 000 USD per year. Neural network has been used. Results have been compared with classic machine learning models: random forest and xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2019)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import learning_curve, train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('train.adult.h5')\n",
    "df['target'] = df['Target'].factorize()[0]\n",
    "df = df.fillna(-1)\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_eng(df):\n",
    "    cat_feats = df.select_dtypes(include=[np.object]).columns\n",
    "    for cat_feat in cat_feats:\n",
    "        df['{0}_cat'.format(cat_feat)] = pd.factorize(df[cat_feat])[0]\n",
    "    df['White'] = df['Race'].apply(lambda x: 1 if x == 'White' else 0)\n",
    "    df['Black'] = df['Race'].apply(lambda x: 1 if x == 'Black' else 0)\n",
    "    df['Other_race'] = df['Race'].apply(\n",
    "        lambda x: 1 if (x != 'White') & (x != 'Black') else 0)\n",
    "    df['Extra_hours'] = df['Hours per week'].map(lambda x: 1 if x > 40 else 0)\n",
    "    df['Extra_hours_num'] = df['Hours per week'].map(\n",
    "        lambda x: x-40 if x > 40 else 0)\n",
    "    df['Husband'] = df['Relationship'].apply(\n",
    "        lambda x: 1 if x == 'Husband' else 0)\n",
    "    df['Married-civ-spouse'] = df['Martial Status'].apply(\n",
    "        lambda x: 1 if x == 'Married-civ-spouse' else 0)\n",
    "    df['Never-married'] = df['Martial Status'].apply(\n",
    "        lambda x: 1 if x == 'Never-married' else 0)\n",
    "    df['Country_us'] = df['Country'].apply(\n",
    "        lambda x: 1 if x == 'United-States' else 0)\n",
    "    df['Country_other'] = df['Country'].apply(\n",
    "        lambda x: 1 if (x != 'United-States') else 0)\n",
    "    df['Occ_white'] = pd.factorize(df[['Occupation_cat', 'White']].apply(\n",
    "        lambda x: '{0}-{1}'.format(x['Occupation_cat'], x['White']), axis=1))[0]\n",
    "    df['Occ_other'] = pd.factorize(df[['Occupation_cat', 'Other_race']].apply(\n",
    "        lambda x: '{0}-{1}'.format(x['Occupation_cat'], x['Other_race']), axis=1))[0]\n",
    "    df['Productive_age'] = df['Age'].apply(\n",
    "        lambda x: 1 if (x >= 24) & (x <= 70) else 0)\n",
    "    df['Master_bachelor'] = df['Education'].apply(\n",
    "        lambda x: 1 if (x == 'Bachelors') | (x == 'Masters') else 0)\n",
    "    df['Doctor_prof'] = df['Education'].apply(\n",
    "        lambda x: 1 if (x == 'Prof-school') | (x == 'Doctorate') else 0)\n",
    "    df['White_husband'] = df[['Relationship', 'Race']].apply(lambda x: 1 if (\n",
    "        x['Relationship'] == 'Husband') & (x['Race'] == 'White') else 0, axis=1)\n",
    "    df['Black_husband'] = df[['Relationship', 'Race']].apply(lambda x: 1 if (\n",
    "        x['Relationship'] == 'Husband') & (x['Race'] == 'Black') else 0, axis=1)\n",
    "    df['Occ_sex'] = pd.factorize(df[['Occupation_cat', 'Sex']].apply(\n",
    "        lambda x: '{0}-{1}'.format(x['Occupation_cat'], x['Sex']), axis=1))[0]\n",
    "    df['Occ_rel_sex'] = pd.factorize(df[['Occupation_cat', 'Relationship_cat', 'Sex']].apply(\n",
    "        lambda x: '{0}-{1}-{2}'.format(x['Occupation_cat'], x['Relationship_cat'], x['Sex']), axis=1))[0]\n",
    "    df['Married_productive'] = pd.factorize(df[['Married-civ-spouse', 'Productive_age']].apply(\n",
    "        lambda x: '{0}-{1}'.format(x['Married-civ-spouse'], x['Productive_age']), axis=1))[0]\n",
    "    df['Occ_martial'] = pd.factorize(df[['Occupation_cat', 'Martial Status']].apply(\n",
    "        lambda x: '{0}-{1}'.format(x['Occupation_cat'], x['Martial Status']), axis=1))[0]\n",
    "    df['Educ_martial'] = df[['Education', 'Martial Status']].apply(lambda x: 1 if (\n",
    "        ((x['Education'] == 'Assoc-voc') | (x['Education'] == 'Bachelors')) & (x['Martial Status'] == 'Married-AF-spouse')) else 0, axis=1)\n",
    "    df['fnlwgt_log'] = np.log2(df['fnlwgt']+1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_full = feat_eng(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network has been build in order to compare result with previous models. Data has been transformed and normalized to fulfill requirements of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = ['Age', 'Education-Num', 'Extra_hours',\n",
    "        'Husband','Married-civ-spouse','Never-married',\n",
    "        'Occupation_cat','Occ_white','Occ_other',\n",
    "        'Master_bachelor','Doctor_prof','Productive_age',\n",
    "        'Capital Gain', 'Capital Loss','Relationship_cat',\n",
    "        'White_husband','Black_husband',\n",
    "        'Sex','White','Black','Other_race',\n",
    "        'Country_us','Country_other','fnlwgt_log',\n",
    "        'Occ_sex','Occ_rel_sex','Married_productive',\n",
    "        'Occ_martial','Martial Status_cat', 'Educ_martial']\n",
    "\n",
    "df_all = df_full[feats]\n",
    "y_all = df_full['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features for one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [ 'Education-Num', 'Husband', 'Married-civ-spouse',\n",
    "           'Never-married', 'Occupation_cat', 'Occ_white', 'Occ_other',\n",
    "           'Master_bachelor', 'Doctor_prof', 'Productive_age', 'Relationship_cat',\n",
    "           'White_husband', 'Black_husband','Sex', 'White', 'Black', 'Other_race',\n",
    "           'Country_us', 'Country_other', 'Occ_sex', 'Occ_rel_sex', 'Married_productive',\n",
    "           'Occ_martial', 'Martial Status_cat', 'Educ_martial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat in features:\n",
    "    df_all[feat] = df_all[feat].astype('object')\n",
    "    \n",
    "cat = df_all.select_dtypes(include=['object']).columns\n",
    "num = df_all.select_dtypes(exclude=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all[num] = StandardScaler().fit_transform(df_all[num])\n",
    "\n",
    "for feat in cat:\n",
    "    df_all = pd.concat([df_all, pd.get_dummies(df_all[feat].astype('category'))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in cat:\n",
    "    del df_all[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32537 entries, 0 to 32536\n",
      "Columns: 413 entries, Age to 1\n",
      "dtypes: float64(5), uint8(408)\n",
      "memory usage: 13.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe has been split into train, test and valid set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22775, 413)   (22775, 1)\n",
      "(4881, 413)   (4881, 1)\n",
      "(4881, 413)   (4881, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_t, y_train, y_t = train_test_split(df_all, y_all, test_size=0.3, random_state=2018) \n",
    "\n",
    "X_train = X_train.reset_index()\n",
    "y_train = y_train.reset_index()\n",
    "X_t = X_t.reset_index()\n",
    "y_t = y_t.reset_index()\n",
    "\n",
    "del X_train['index']\n",
    "del X_t['index']\n",
    "del y_train['index']\n",
    "del y_t['index']\n",
    "\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_t, y_t, test_size=0.5, random_state=2018) \n",
    "\n",
    "X_test = X_test.reset_index()\n",
    "y_test = y_test.reset_index()\n",
    "X_valid = X_valid.reset_index()\n",
    "y_valid = y_valid.reset_index()\n",
    "\n",
    "del X_valid['index']\n",
    "del X_test['index']\n",
    "del y_test['index']\n",
    "del y_valid['index']\n",
    "\n",
    "print(X_train.shape, ' ', y_train.shape)\n",
    "print(X_valid.shape, ' ', y_valid.shape)\n",
    "print(X_test.shape, ' ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 100\n",
    "n_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"gsn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"h1\",\n",
    "                              activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"h2\",\n",
    "                              activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"h3\",\n",
    "                              activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(hidden3, n_outputs, name=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"learn\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"estimation\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_board\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir = log_dir(\"earning_gsn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(x_=X_train.values, y_=y_train.values.ravel(), batch_size=300, batch_index=2, epocha=1):\n",
    "    \n",
    "    tf.set_random_seed(epoch*len(x_)//batch_size + batch_index)\n",
    "    np.random.seed(epocha*len(x_)//batch_size + batch_index)\n",
    "    \n",
    "    indices = np.random.randint(len(x_), size=batch_size) \n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return x_[indices], y_[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation: 84.696% \tLoss: 0.33120\n",
      "Epoch: 2 \tValidation: 84.860% \tLoss: 0.32741\n",
      "Epoch: 4 \tValidation: 84.204% \tLoss: 0.32549\n",
      "Epoch: 6 \tValidation: 85.085% \tLoss: 0.31937\n",
      "Epoch: 8 \tValidation: 85.187% \tLoss: 0.31897\n",
      "Epoch: 10 \tValidation: 85.515% \tLoss: 0.31955\n",
      "Epoch: 12 \tValidation: 85.208% \tLoss: 0.31484\n",
      "Epoch: 14 \tValidation: 85.065% \tLoss: 0.31884\n",
      "Epoch: 16 \tValidation: 84.942% \tLoss: 0.31810\n",
      "Epoch: 18 \tValidation: 85.597% \tLoss: 0.31434\n",
      "Epoch: 20 \tValidation: 85.392% \tLoss: 0.31575\n",
      "Epoch: 22 \tValidation: 85.413% \tLoss: 0.31795\n",
      "Epoch: 24 \tValidation: 85.392% \tLoss: 0.31588\n",
      "Early stop\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "n_batches = 100\n",
    "batch_s = X_train.shape[0]//n_batches\n",
    "\n",
    "checkpoint_path = \"/tmp/my_gsn_earning.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_gsn_earning\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Learn interrupted. Back to epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(n_batches ):\n",
    "            X_batch, y_batch = next_batch(batch_index=iteration, epocha=epoch, batch_size=batch_s)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary],\n",
    "                                                                                  feed_dict={X:X_valid.values, y:y_valid.values.ravel()})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation: {:.3f}%\".format(accuracy_val * 100),\"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 2\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stop\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_gsn_earning\n",
      "0.85576725\n"
     ]
    }
   ],
   "source": [
    "os.remove(checkpoint_epoch_path)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X: X_test.values, y: y_test.values.ravel()})\n",
    "print(accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are comparable, however a little bit worse than using RandomForestClassifier (about 86% of accuracy) and XGBClassifier (about 87% of accuracy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "logdir = log_dir(\"earning_gsn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='learn')\n",
    "\n",
    "dropout_rate = 0.6 \n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"h1\",\n",
    "                              activation=tf.nn.elu,kernel_initializer=he_init)\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, name=\"h2\",\n",
    "                              activation=tf.nn.elu,kernel_initializer=he_init)\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "\n",
    "    hidden3 = tf.layers.dense(hidden2_drop, n_hidden3, name=\"h3\",\n",
    "                              activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(hidden3, n_outputs, name=\"output\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"learn\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"validation\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation: 84.532% \tLoss: 0.33293\n",
      "Epoch: 2 \tValidation: 84.778% \tLoss: 0.32966\n",
      "Epoch: 4 \tValidation: 84.163% \tLoss: 0.32873\n",
      "Epoch: 6 \tValidation: 85.003% \tLoss: 0.32100\n",
      "Epoch: 8 \tValidation: 85.208% \tLoss: 0.31999\n",
      "Epoch: 10 \tValidation: 85.290% \tLoss: 0.32120\n",
      "Epoch: 12 \tValidation: 85.474% \tLoss: 0.31580\n",
      "Epoch: 14 \tValidation: 85.310% \tLoss: 0.32002\n",
      "Epoch: 16 \tValidation: 84.860% \tLoss: 0.31989\n",
      "Epoch: 18 \tValidation: 85.720% \tLoss: 0.31539\n",
      "Epoch: 20 \tValidation: 85.351% \tLoss: 0.31734\n",
      "Epoch: 22 \tValidation: 85.146% \tLoss: 0.31961\n",
      "Epoch: 24 \tValidation: 85.433% \tLoss: 0.31667\n",
      "Early stop\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 50\n",
    "n_batches = 100\n",
    "batch_s=X_train.shape[0] // n_batches\n",
    "\n",
    "checkpoint_path = \"/tmp/my_gsn_earning.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_gsn_earning\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 10\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Learn interrupted. Back to epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = next_batch(batch_index=iteration,epocha=epoch,batch_size=batch_s)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary],\n",
    "                                                                                  feed_dict={X:X_valid.values, y:y_valid.values.ravel()})\n",
    "\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 2\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stop\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_gsn_earning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8543331"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove(checkpoint_epoch_path)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X: X_test.values, y: y_test.values.ravel()})\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with dropout are not better than using initial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "logdir = log_dir(\"earning_gsn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='learn')\n",
    "\n",
    "with tf.name_scope(\"gsn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "            training=training, momentum=batch_norm_momentum)\n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"h1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"h2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    hidden3 = my_dense_layer(bn2, n_hidden3, name=\"h3\")\n",
    "    bn3 = tf.nn.elu(my_batch_norm_layer(hidden3))\n",
    "    logits_before_bn = my_dense_layer(bn3, n_outputs, name=\"output\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)    \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "    \n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"learn\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"validation\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation: 84.798% \tLoss: 0.33731\n",
      "Epoch: 2 \tValidation: 84.737% \tLoss: 0.33159\n",
      "Epoch: 4 \tValidation: 84.634% \tLoss: 0.32991\n",
      "Epoch: 6 \tValidation: 84.675% \tLoss: 0.32647\n",
      "Epoch: 8 \tValidation: 84.942% \tLoss: 0.32500\n",
      "Epoch: 10 \tValidation: 85.065% \tLoss: 0.32630\n",
      "Epoch: 12 \tValidation: 85.187% \tLoss: 0.32355\n",
      "Epoch: 14 \tValidation: 84.962% \tLoss: 0.32935\n",
      "Epoch: 16 \tValidation: 85.310% \tLoss: 0.32451\n",
      "Epoch: 18 \tValidation: 84.839% \tLoss: 0.32873\n",
      "Epoch: 20 \tValidation: 84.614% \tLoss: 0.33626\n",
      "Epoch: 22 \tValidation: 84.614% \tLoss: 0.33357\n",
      "Early stop\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 50\n",
    "n_batches = 100\n",
    "batch_s=X_train.shape[0] // n_batches\n",
    "\n",
    "checkpoint_path = \"/tmp/my_gsn_earning.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_gsn_earning\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 10\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Learn interrupted. Back to epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for iteration in range(n_batches):\n",
    "            X_batch, y_batch = next_batch(batch_index=iteration,epocha=epoch,batch_size=batch_s)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True,X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], \n",
    "                                                                                  feed_dict={X:X_valid.values, y:y_valid.values.ravel()})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 2\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stop\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_gsn_earning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8531039"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.remove(checkpoint_epoch_path)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X: X_test.values, y: y_test.values.ravel()})\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization has no significant impact on result in this case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
